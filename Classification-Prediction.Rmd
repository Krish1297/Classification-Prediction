---
title: "QDV23_Rskript_SupervisedLearningClassificationPrediction"
author: "Krishnamoorthy Juttoo Chandrasekaran"
date: "22/10/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This week we want to look at classification. To do this, we want to predict in a customer dataset which customers will react to a discount promotion and actually buy/convert.
This boils down to a categorical dependent variable (purchase, or not).

First we load the new data set


## Load Data



```{r}
rm(list = ls()) #clear the workspace


Promo.data <- read.csv("PromoCampaign.csv", header=TRUE, sep=",",  dec=".") #load dataset
Promo.data <- Promo.data[Promo.data$offer == "Discount", ] #We only want to look at customers who actually got a discount offer


head(Promo.data) #look at the first few rows

```

This data set consists of client history data of a service provider.
Each row is a client.
We are particularly interested in:
 * conversion: 1: yes, or 0: no.

To do this, we want to see if the discount level, and some other customer information have an effect:
 * discountlevel : the randomly assigned amount of discount in percentage points.
 * recency: how many days ago was the last purchase?
 * history: cumulative usage of the service
 * used_discount : has used a discount before
 * used_bogo : has used a buy-one-get-one-free before
 * zip_code: Place of residence, only distinguished between rural, suburban, and inner city.
 * is_referral: did the customer come through another customer's referral?
 * channel: Through which channel did the client come? (web, phone, multichannel)


# Part I: Prediction rules

Let's already load a few packages for classification functions, plotting and prediction models

```{r}
if(!require(caret)){install.packages("caret")};library(caret) # package "classification and regression training" with useful functions

if(!require(tidyverse)){install.packages("tidyverse")};library(tidyverse) #Package for fancy graphs and working with data

```
## Question: Does the discount level predict conversion?

A short plot of the empirical density distributions clearly shows that the converted customers are mainly at the high discounts:


```{r}
#Plot the empirical density of the DV values over discount levels to see if there are distinct regimes
ggplot(Promo.data, aes(x = discountlevel)) +
  geom_density(aes(color = as.factor(conversion)), size=1, position = "identity") +
  scale_color_manual(values = c("#00AFBB", "#E7B800")) +
  theme_classic() + 
  theme(legend.position = "top")
```
So it stands to reason that we try to predict the conversion with the discount level (and perhaps other variables). 

## make a simple, rule-based prediction

First, we make a simple prediction rule: If the discount is above 25%, then we predict a conversion, otherwise not.

```{r}
Promo.data$pred.conversion <- ifelse(Promo.data$discountlevel < 18.6, "1", "0") #predict conversion (if discount is > 25%)

```

But how good is this simple rule? Let's look at the accuracy of the prediction:


## Accuracy of the prediction

For this we need a "confusion table" from which we can calculate the accuracy. There are many related measures for comparing predictions, but most of them are based on a confusion table.

You may already know the idea of a confusion table in principle from the error of the first and second kind (alpha and beta values) in hypothesis testing: A confusion table is a table with the true positive, true negative, false positive and false negative predicted cases.

We use the confusionMatrix() function for an output of typical measures of goodness.

```{r}

confusionMatrix(as.factor(Promo.data$pred.conversion),as.factor(Promo.data$conversion), mode="everything") #calculate accuracy (and a few more machine learning metrics)


```

Here you will find typical quality measures, such as:
 * accuracy (#correct / #total)
 * sensitivity = recall = TPR = TP/(TP+FN)
 * Specificity = TNR = TN/(TN+FP)
 * Positive Predictive Value = Precision = TP/(TP+FP)

These quality measures are all calculated from the confusion table.
With:
 * TP = true positive
 * TN = true negative
 * FP = false positive
 * FN = false negative
 * TPR = true positive ratio
 * TNR = true negative ratio


Our simple rule-based prediction model actually looks pretty good. 

Can you find a better rule-based prediction model? What is a good prediction criterion? Try it out!


```{r}
# your code

```


# PART 2: Automatic decision rules: conversion prediction with a model

I don't know about you, but I now ask myself: can I find an automatic way to find a good prediction rule?
To do this is exactly the task of a classification model.
There are many different models for this, and in this course you will learn about several of them, like:

 * Classification trees (CART)
 * Random Forests
 * Logistic regression
 * Neural Networks

We will start with a simple classification tree, because this model mimics what we have just done manually: we build a decision tree with rules that we can read off.

# PART 2a: Classification trees (CART)

Now let's see if we can find an automatic prediction rule and beat our manual prediction with a decision tree according to CART.

To do this, we first load two packages

```{r}
if(!require(rpart)){install.packages("rpart")};library(rpart) # package "rpart" for classification trees
if(!require(rpart.plot)){install.packages("rpart.plot")};library(rpart.plot) # package "rpart.plot" for plotting classification trees
```


Now we build a first small decision tree and plot it afterwards.
Note, that we include a number of possible decision variables in the model. MAybe we do not yet know, which variable is a good one, or maybe the decision tree is really complicated and we want to see, which variables are important after each subsequent decision.

```{r}
cart0 <- rpart(Purchase_intention ~ Age_cat + Sex_1m + Age  + Education  + CategoryKnowledge + Conversation + Conversation_DUM + Social_network + Social_network_DUM + TV_ad + TV_ad_DUM + BrandRecognition + Purchased , fullData, 
              method = "class")
summary(cart0)

#A plot of the tree
prp(cart0, 
    type = 1, 
    extra = 4, 
digits=3)
```

Aha, with these standard setting swe find only one relevant variable, "discount". And the decision rule is very likely close to what you already found manually.


But wait!
We haven't made any real predictions at all. So far we have only determined the best fit within the sample and then determined the deviations again within the sample. That is not yet real prediction. We have to do that differently.

To really see if a model can predict, the model must not know the data on which it is tested. So we have to divide the data into a training and a test set.

## Training and predicting: Switching to a machine learning paradigm

From now on, we learn in training data and validate in test data

Now we do it all again, but this time we divide the dataset into a training set (for estimation) and a test set (for prediction). Let us use only the first 10,000 cases for training and the rest for testing (about 50/50).

```{r}
#splitting the data in train and test
data_train <- Promo.data[1:10000,]
data_test <- Promo.data[10001:nrow(Promo.data),]


```


## build tree (CART) only from training data

Now we build a first small decision tree only on the train data and plot it afterwards.

```{r}
cart1 <- rpart(conversion ~ discountlevel + recency + history + used_discount + used_bogo, data_train, 
              method = "class")
summary(cart1)

#A plot of the tree
prp(cart1, 
    type = 0, 
    extra = 4, 
    digits = 3)


```
The best split (and the only relevant one...) is determined by the discount level. All those with a discount higher than 18.7% are predicted to be converters. This is very similar to what we found before, on the full sample (18.6), but not exactly the same. This is because the sample is different, and the tree is built differently.

Let's see if this prediction holds true:

## predict with tree in test data

```{r}
#estimating on the test set
probability.tree = predict(cart1, data_test)[,2] #here we get the probability of conversion for each case in the test data


data_test$pred.conversion.tree1 <- ifelse(probability.tree > 0.5, "1", "0") #predict conversion (if probability is > 50%)


confusionMatrix(as.factor(data_test$pred.conversion.tree1),as.factor(data_test$conversion), mode="everything") #calculate 

```

Yes, this looks very accurate.

Only the tree is a bit boringly small. For fun, even if it doesn't make the model better, we can make the tree a bit more complicated:

## make the tree more complicated?

Let us just force the tree to have more splits. We do this by setting the minimum number of cases in a node to 50 and the minimum number of cases in a leaf to 30. This will force the tree to be more granular. We also set the complexity parameter to .001, which will force the tree to be more complicated.

```{r}
cart2 <- rpart(conversion ~ discountlevel + recency + history + used_discount + used_bogo, data_train, 
              method = "class",
              control = rpart.control(minsplit = 50, minbucket = 30, cp = .001)) #this will force a more granular tree
summary(cart2)

prp(cart2, 
    type = 0, 
    extra = 4, 
    digits = 3)
```

The tree is now more complicated. But is it better?

```{r}
#estimating on the test set
probability.tree = predict(cart2, data_test)[,2]


data_test$pred.conversion.tree2 <- ifelse(probability.tree > 0.5, "1", "0") #predict conversion (if probability is > 50%)


confusionMatrix(as.factor(data_test$pred.conversion.tree2),as.factor(data_test$conversion), mode="everything") #calculate 

```
The False Positives have decreased somewhat and the False Negatives have increased somewhat. Overall, however, the result is not worth the more complicated tree. Generally, when a model shows similar results with different levels of complexity, it is better to choose the simpler model. This is called "Occam's Razor".


## model accuracy comparison table

But we haven't really compared the models yet. Let's build a table of the outputs side by side:


```{r}
# lets store the accuracy measures into objects:

# manual prediction
matrix1 <- confusionMatrix(as.factor(data_test$pred.conversion),as.factor(data_test$conversion), mode="everything")
# a simple CART tree
matrix2 <- confusionMatrix(as.factor(data_test$pred.conversion.tree1),as.factor(data_test$conversion), mode="everything")
# a complicated CART tree
matrix3 <- confusionMatrix(as.factor(data_test$pred.conversion.tree2),as.factor(data_test$conversion), mode="everything")



# And now we put them into a table:
comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4), #rounding the numbers to 4 digits
                                  round(c(matrix2$overall,matrix2$byClass),digit=4),
                                  round(c(matrix3$overall,matrix3$byClass),digit=4)
                                  )
                            )
names(comparison) <- c("manual","CART-simple","CART-complicated") #name of the columns

comparison #output
 
```

The simple tree is the best model. It is also the simplest model. So we will use it for the rest of the analysis. 
But quite often you will find many different trees that work well. Which one is most robust to changes in the data? We could test this through cross-validating many different trees. And how would we do and call that (many trees is what...)? We could do a random forest.


# PART 2b: Random Forest

Let us load a package for random forest models and build a random forest model. We will use 100 trees and 2 variables per tree. We will also ask the model to calculate variable importance. This will tell us which variables are most important for the prediction.

```{r}
if(!require(randomForest)){install.packages("randomForest")};library(randomForest) # package for random forest models

rf1 = randomForest(conversion ~ ., data=data_train[,2:11], ntree=100, mtry=2, importance=TRUE) #train the model on all variables that are not the conversion variable
rf1
```
Now, let us check how well this random forest model does:

```{r}
probability.rf1 <- predict(rf1, data_test, type="response")
data_test$pred.conversion.rf1 <- ifelse(probability.rf1 > 0.5, "1", "0") #predict conversion (if probability is > 50%)

confusionMatrix(as.factor(data_test$pred.conversion.rf1),as.factor(data_test$conversion), mode="everything") #calculate 


```

That looks like the best so far. But let us compare the models:

```{r}
# lets store the accuracy measures into objects:

# random forest
matrix4 <- confusionMatrix(as.factor(data_test$pred.conversion.rf),as.factor(data_test$conversion), mode="everything")


# And now we put them into a table:
comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4), #rounding the numbers to 4 digits
                                  round(c(matrix2$overall,matrix2$byClass),digit=4),
                                  round(c(matrix3$overall,matrix3$byClass),digit=4),
                                  round(c(matrix4$overall,matrix4$byClass),digit=4)
                                  )
                            )
names(comparison) <- c("manual","CART-simple","CART-complicated","random forest") #name of the columns

comparison #output
 
```
Overall, it seems like the best model is the random forest model. It is also the most complicated model. And there is one other issue:
It is a black-box model.
Like other machine learning models it does not really tell us what is going on. It is a black box.
Perhaps a slight insight into the model can be gained by looking at the variable importance:


```{r}
varImpPlot(rf1, main="Importance of Variables in the Random Forest")
```
As before, across most trees in the forest, and presumably within these trees, the discount level is the most important variable. But the other variables also play some role.

If you want, you can play around with other variables or settings in your random forest model. 

```{r}

```


There is an important lesson here for us: interpretability is always an issue with machine learning models and AI.
While prediction is often superior, we often lose the ability to understand what is going on. This is a trade-off that we need to be aware of. As a data analyst, you will often be asked to explain what is going on. And you will often not be able to do so. This is a problem. But it is also a problem that we can solve. We can use machine learning models to predict, and then use simpler models to explain what is going on. This is called "model stacking" and it is a very powerful technique.

Next, we will look at a "classic" model that allows easier interpretation: logistic regression.

# PART 3: Logistic Regression

Regression models are very easy to interpret. They are also very easy to use. Later in this course, we will use a lot of regression models. And actually, we have already used a regression model in other exercises: the linear regression model, when we calculated a group comparison in weighted or matched data. 

But the linear regression model is not appropriate for our data. Why not? Because our dependent variable is not normally distributed. It is categorical. So we need a different regression model. We need a logistic regression model.

To illustrate, let us build a linear regression model with discount level as independent variable first:


##linear model:
```{r}
library(ggplot2)
#Plot the DV over discount levels, using a linear model function
gg_ad <- ggplot(data_train, aes(x=discountlevel, y=conversion)) + 
  geom_point() +
  geom_smooth(method=lm, se=FALSE, ) +
  theme_light() +
  ggtitle("Discount Level and Conversions (linear function)") +
  xlab("Discount Level") +
  ylab("Conversion (1 = yes, 0 = no)") +
  theme(plot.title = element_text(hjust = 0.5))
gg_ad



```
That doesn't look so good, especially because our estimated straight line predicts lots of negative conversions (which can't exist). No wonder, after all, our dependent variable is actually categorical (1/0) and thus not normally distributed.


With a logistic model (instead of a linear model) we can model the data much better:

```{r}


#Plot the DV over discount levels, using a logit model function
gg_ad2 <- ggplot(data_train, aes(x=discountlevel, y=conversion)) + 
  geom_point() +
  geom_smooth(method=glm, method.args = list(family = "binomial"), se=FALSE, ) +
  theme_light() +
  ggtitle("Discount Level and Conversions (logit function)") +
  xlab("Discount Level") +
  ylab("Conversion (1 = yes, 0 = no)") +
  theme(plot.title = element_text(hjust = 0.5))
gg_ad2

```

The logistic model is THE choice model, and thus also the standard classification model of the old school. 

In the background of a logistic model, the probability of a conversion is estimated as a linear model, but not the conversion itself. The result is an S-shaped progression of the estimated function. The probability of a conversion is estimated as a function of the discount level. The estimated function is then used to predict the probability of a conversion for each discount level. The predicted probability is then compared with the actual conversion. The model is then optimized so that the predicted probabilities are as close as possible to the actual conversions.

##Estimating the logit model

In R, we can build the logit model similarly to a linear model, but we need the glm() (general linear model) command with the family suffix "binomial" (because the conversions, with only two possible values, are binomially distributed):

```{r}

m1 <- glm(conversion ~ discountlevel, data_train, family = "binomial")
summary(m1)
```
We find a significant, positive influence of the discount level. The higher the discount level, the higher the probability of a conversion. This is not surprising, because we have already seen this in the descriptive statistics.

## multivariate logit model 
Now we test whether the other covariates could also have an influence:

```{r}
m2 <- glm(conversion ~ discountlevel + recency + history + used_discount + used_bogo+ is_referral+as.factor(channel) + as.factor(zip_code), data_train , family = "binomial")
summary(m2)
```



Except for "history" and "bogo", all other variables tested have some significant impact on conversion.

What we don't see here, however, is how well this model predicts the actual conversions.



```{r}
logit.probability <- predict(m2, data_test, type = "response") #calculate estimated probabilities that an observation is a conversion

data_test$pred.conversion.logit <- ifelse(logit.probability > 0.5, "1", "0") #predict conversion (if probability is > 50%)


confusionMatrix(as.factor(data_test$pred.conversion.logit),as.factor(data_test$conversion), mode="everything") #calculate accuracy (and a few more machine learning metrics)
```
Let us add the model to our comparison table


```{r}
# lets store the accuracy measures into objects:

# random forest
matrix5 <- confusionMatrix(as.factor(data_test$pred.conversion.logit),as.factor(data_test$conversion), mode="everything")


# And now we put them into a table:
comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4), #rounding the numbers to 4 digits
                                  round(c(matrix2$overall,matrix2$byClass),digit=4),
                                  round(c(matrix3$overall,matrix3$byClass),digit=4),
                                  round(c(matrix4$overall,matrix4$byClass),digit=4),
                                  round(c(matrix5$overall,matrix5$byClass),digit=4)
                                  )
                            )
names(comparison) <- c("manual","CART-simple","CART-complicated","random forest", "logit") #name of the columns

comparison #output
 
```
Wow, this is a really good model! It is even better than the random forest model. And it is much better than the simple CART model. It is also better than the complicated CART model. And it is better than the manual model. And it is interpretable. Good choice.



#PART 4: single-layer neural nets

Now we want to go a bit ahead and see if we can also classify with the help of a simple neural network.
For this we need a new package again

```{r}
if(!require(nnet)){install.packages("nnet")};library(nnet) # package "nnet" for single-layer neural nets
```


##train neural net on test data

Now we learn a very simple neural network, with a layer of 6 neurons ("hidden layer") between the inputs and our output.

```{r}
m.neural <-nnet(conversion ~ discountlevel + recency + history + used_discount + used_bogo+ is_referral+as.factor(channel) + as.factor(zip_code), data=data_train, #1 output, 10 inputs
                size = 10, #hiddenlayer has 10 neurons
                decay = 0.0001, #regularisation to counter overfitting (should be smaller with larger training data)
                maxit = 500) #max number of iterations

summary(m.neural)

```
Our neural network has converged on a solution and found weights for the paths between inputs, neurons, and output.
Now we predict with this:

#predict in test data

```{r}

probability.neural = predict(m.neural,data_test,type="raw")


data_test$pred.conversion.neural<- ifelse(probability.neural > 0.5, "1", "0") #predict conversion (if probability is > 50%)


confusionMatrix(as.factor(data_test$pred.conversion.neural),as.factor(data_test$conversion), mode="everything") #calculate 

  
  
```
Apparently, our simple neural network is as good as the two previously tested learning models. Fine.

Let us add the model to our comparison table
```{r}
# lets store the accuracy measures into objects:

# random forest
matrix6 <- confusionMatrix(as.factor(data_test$pred.conversion.neural),as.factor(data_test$conversion), mode="everything")


# And now we put them into a table:
comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4), #rounding the numbers to 4 digits
                                  round(c(matrix2$overall,matrix2$byClass),digit=4),
                                  round(c(matrix3$overall,matrix3$byClass),digit=4),
                                  round(c(matrix4$overall,matrix4$byClass),digit=4),
                                  round(c(matrix5$overall,matrix5$byClass),digit=4),
                                  round(c(matrix6$overall,matrix6$byClass),digit=4)
                                  )
                            )
names(comparison) <- c("manual","CART-simple","CART-complicated","random forest", "logit", "neural") #name of the columns

comparison #output
 
```



# PART 5: Bonus task and presentation task




## Bonus task 3: targeting cleaning product users

This week your task is again quite short:
 * Take the pre-post media survey dataset (MediaContactSurveyPrePost).
 * Create a training set of all first wave users (the rest will be test dataset).
 * We want to predict which members of the second wave buy. To do this, 
 * Check a few classification models using wave 1 variables (e.g. CART, random forest, neural net, logit, etc.)
 * Create a comparison table of the prediction measures of all models you tested.
 * Save the table as a pdf or image
 * Upload the file to moodle




```{r}
# create your final plot here
#splitting the data in train and test

#PrePostMediaSurvey.data <- as.data.frame(read_xlsx("MediaContactSurveyPrePost.xlsx", sheet= 1, col_names = TRUE))
#data_train <-  PrePostMediaSurvey.data[PrePostMediaSurvey.data$SurveyWave == 1, ]
#data_test <- PrePostMediaSurvey.data[PrePostMediaSurvey.data$SurveyWave == 2, ]
#ggplot(PrePostMediaSurvey.data, aes(x = BrandRecognition)) +
#  geom_density(aes(color = as.factor(Purchase_intention)), size=1, position = "identity") +
#  scale_color_manual(values = c("#00AFBB", "#E7B800")) +
 # theme_classic() + 
#  theme(legend.position = "top")
#data_test$pred.BrandRecognition <- ifelse(data_test$BrandRecognition >= 8, "1", "0")
#confusionMatrix(as.factor(PrePostMediaSurvey.data$pred.BrandRecognition),as.factor(PrePostMediaSurvey.data$Purchase_intention), mode="everything") #calculate accuracy (and a few more machine learning metrics)
#cart0 <- rpart(Purchase_intention ~ Age_cat + Sex_1m + Age  + Education  + CategoryKnowledge + Conversation + Conversation_DUM + Social_network + Social_network_DUM + TV_ad + TV_ad_DUM + BrandRecognition + Purchased , PrePostMediaSurvey.data,             method = "class")
#summary(cart0)
#cart1 <- rpart(Purchase_intention ~ Age_cat + Sex_1m + Age  + Education  + CategoryKnowledge + Conversation + Conversation_DUM + Social_network + Social_network_DUM + TV_ad + TV_ad_DUM + BrandRecognition + Purchased , data_train,             method = "class")
#summary(cart1)

#A plot of the tree
#prp(cart1, type = 0, extra = 4, digits = 3)
#A plot of the tree
#prp(cart0, type = 1, extra = 4, digits=3)
#probability.tree = predict(cart1, data_test)[,2] #here we get the probability of conversion for each case in the test data


#data_test$pred.BrandRecognition.tree1 <- ifelse(probability.tree > 0.5, "1", "0") #predict conversion (if probability is > 50%)


#confusionMatrix(as.factor(data_test$pred.BrandRecognition.tree1),as.factor(data_test$Purchase_intention), mode="everything") #calculate 
#cart2 <- rpart(Purchase_intention ~ Age_cat + Sex_1m + Age  + Education  + CategoryKnowledge + Conversation + Conversation_DUM + Social_network + Social_network_DUM + TV_ad + TV_ad_DUM + BrandRecognition + Purchased , data_train, method = "class",control = rpart.control(minsplit = 50, minbucket = 30, cp = .001)) #this will force a more granular tree
#summary(cart2)

#prp(cart2, type = 0, extra = 4, digits = 3)
#probability.tree = predict(cart2, data_test)[,2]


#data_test$pred.BrandRecognition.tree2 <- ifelse(probability.tree > 0.5, "1", "0") #predict conversion (if probability is > 50%)


#confusionMatrix(as.factor(data_test$pred.BrandRecognition.tree2),as.factor(data_test$Purchase_intention), mode="everything") 
# lets store the accuracy measures into objects:

# manual prediction
#matrix1 <- confusionMatrix(as.factor(data_test$pred.BrandRecognition),as.factor(data_test$Purchase_intention), mode="everything")
# a simple CART tree
#matrix2 <- confusionMatrix(as.factor(data_test$pred.BrandRecognition.tree1),as.factor(data_test$Purchase_intention), mode="everything")
# a complicated CART tree
#matrix3 <- confusionMatrix(as.factor(data_test$pred.BrandRecognition.tree2),as.factor(data_test$Purchase_intention), mode="everything")
# And now we put them into a table:
#comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4), round(c(matrix2$overall,matrix2$byClass),digit=4),round(c(matrix3$overall,matrix3$byClass),digit=4))                   )
#names(comparison) <- c("manual","CART-simple","CART-complicated") #name of the columns

#comparison #output
#data_train <- na.omit(data_train)
#rf1 = randomForest(Purchase_intention ~ ., data=data_train[,2:16], ntree=100, mtry=2, importance=TRUE) #train the model on all variables that are not the conversion variable
#rf1
#probability.rf1 <- predict(rf1, data_test, type="response")
#data_test$pred.BrandRecognition.rf1 <- ifelse(probability.rf1 > 0.5, "1", "0") #predict conversion (if probability is > 50%)

#confusionMatrix(as.factor(data_test$pred.BrandRecognition.rf1),as.factor(data_test$Purchase_intention), mode="everything")
#matrix4 <- confusionMatrix(as.factor(data_test$pred.BrandRecognition.rf1),as.factor(data_test$Purchase_intention), mode="everything")


# And now we put them into a table:
#comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4),round(c(matrix2$overall,matrix2$byClass),digit=4),round(c(matrix3$overall,matrix3$byClass),digit=4),round(c(matrix4$overall,matrix4$byClass),digit=4)                       ))
#names(comparison) <- c("manual","CART-simple","CART-complicated","random forest") #name of the columns

#comparison
#varImpPlot(rf1, main="Importance of Variables in the Random Forest")
#gg_ad <- ggplot(data_train, aes(x=BrandRecognition, y=Purchase_intention)) + 
 # geom_point() +
  #geom_smooth(method=lm, se=FALSE, ) +
  #theme_light() +
  #ggtitle("Discount Level and Conversions (linear function)") +
  #xlab("Discount Level") +
  #ylab("Conversion (1 = yes, 0 = no)") +
  #theme(plot.title = element_text(hjust=0.5))
#gg_ad

#gg_ad2 <- ggplot(data_train, aes(x=BrandRecognition, y=Purchase_intention)) + 
  #geom_point() +
  #geom_smooth(method=glm, method.args = list(family = "binomial"), se=FALSE, ) +
  #theme_light() +
  #ggtitle("Discount Level and Conversions (logit function)") +
  #xlab("Discount Level") +
  #ylab("Conversion (1 = yes, 0 = no)") +
  #theme(plot.title = element_text(hjust=0.5))
#gg_ad2
#m1 <- glm(Purchase_intention ~ BrandRecognition, data_train, family = "binomial")
#summary(m1)

#m2 <- glm(Purchase_intention ~ BrandRecognition+ Age_cat + Sex_1m + Age  + Education  + CategoryKnowledge + Conversation + Conversation_DUM + Social_network + Social_network_DUM + TV_ad + TV_ad_DUM + BrandRecognition + Purchased , data_train , family = "binomial")
#summary(m2)
#logit.probability <- predict(m2, data_test, type = "response") #calculate estimated probabilities that an observation is a conversion

#data_test$pred.BrandRecognition.logit <- ifelse(logit.probability > 0.5, "1", "0") #predict conversion (if probability is > 50%)


#confusionMatrix(as.factor(data_test$pred.BrandRecognition.logit),as.factor(data_test$Purchase_intention), mode="everything") #calculate accuracy (and a few more machine learning metrics)

#matrix5 <- confusionMatrix(as.factor(data_test$pred.BrandRecognition.logit),as.factor(data_test$Purchase_intention), mode="everything")


# And now we put them into a table:
#comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4),round(c(matrix2$overall,matrix2$byClass),digit=4),round(c(matrix3$overall,matrix3$byClass),digit=4),round(c(matrix4$overall,matrix4$byClass),digit=4),round(c(matrix5$overall,matrix5$byClass),digit=4)))
#names(comparison) <- c("manual","CART-simple","CART-complicated","random forest", "logit") #name of the columns

#comparison #output
m.neural <-nnet(Purchase_intention ~ BrandRecognition+ Age_cat + Sex_1m + Age  + Education  + CategoryKnowledge + Conversation + Conversation_DUM + Social_network + Social_network_DUM + TV_ad + TV_ad_DUM + BrandRecognition + Purchased , data=data_train, #1 output, 10 inputs
                size = 10, #hiddenlayer has 10 neurons
                decay = 0.0001, #regularisation to counter overfitting (should be smaller with larger training data)
                maxit = 500) #max number of iterations

summary(m.neural)

probability.neural = predict(m.neural,data_test,type="raw")


data_test$pred.BrandRecognition.neural<- ifelse(probability.neural > 0.5, "1", "0") #predict conversion (if probability is > 50%)


confusionMatrix(as.factor(data_test$pred.BrandRecognition.neural),as.factor(data_test$Purchase_intention), mode="everything")#calculate

matrix6 <- confusionMatrix(as.factor(data_test$pred.BrandRecognition.neural),as.factor(data_test$Purchase_intention), mode="everything")


# And now we put them into a table:
comparison <- as.data.frame(cbind(round(c(matrix1$overall,matrix1$byClass),digit=4), #rounding the numbers to 4 digits
                                  round(c(matrix2$overall,matrix2$byClass),digit=4),
                                  round(c(matrix3$overall,matrix3$byClass),digit=4),
                                  round(c(matrix4$overall,matrix4$byClass),digit=4),
                                  round(c(matrix5$overall,matrix5$byClass),digit=4),
                                  round(c(matrix6$overall,matrix6$byClass),digit=4)
                                  )
                            )
names(comparison) <- c("manual","CART-simple","CART-complicated","random forest", "logit", "neural") #name of the columns

comparison #output

# save as png
ggsave("Krishnamoorthy.png", plot = your.plot, device = "png", dpi = 300, width = 10, height = 10, units = "cm", limitsize = FALSE)


```



## Presentation 3 (homework for one of you)

 * You will create a maximum 15 minutes PPT presentation on who is likely to buy the cleaning product from the Pre-post survey data, which you will then present in the next lecture; then we can all ask questions.
 * The premise is that you were tasked with running and analysing the effect of the media campaign (which was run in a large European country before 2020). As described the media campaign had three components (TV, social media, WOM) and the intent was reach, brand recognition and of course purchases. The company wants to know who is likely to buy the cleaning product, so they can target them with a special offer.  
 * think about the SCQA: Situation, Complication, Question, Answer that you want to present
 * You can choose any visualizations you like, but it should be helpful to understand the data. And show at least three visualizations
 * Be prepared for a lot of "why this?" questions from the audience (or me), so you should be able to explain why you chose this visualization, and what it shows. And perhaps prepare an appendix with additional visualizations that you can show if needed.
 * Be even prepared for a few "why not something else?" questions. So maybe you can show why this is the best visualization, and why other visualizations are not as good.
 * Always consider the marking criteria (see below)
 * send me your PPT file before the lecture where you present, so I can upload it to moodle

## Marking criteria for presentations

 - 1) Situation: What is the situation? What is the problem? what is the complication
    * presented interestingly and clearly? -> yes, meh, no.
    * convincing material provided? -> yes, meh, no.
 - 2) What is the goal? what is the question? what is the answer?
    * presented interestingly and clearly? -> yes, meh, no.
    * convincing material provided? -> yes, meh, no.
-  3) Give an interesting punchline or teaser upfront -> yes, meh, no.
-  4) What is the data? what is the evidence? 
    * presented interestingly and clearly? -> yes, meh, no.
    * convincing material provided? -> yes, meh, no.
-  5) what is the analysis? what is done? 
    * presented interestingly and clearly? -> yes, meh, no.
    * convincing material provided? -> yes, meh, no.
-  6) what is the result? What is the discussion? what is the outlook? what is the next step?
    * presented interestingly and clearly? -> yes, meh, no.
    * convincing material provided? -> yes, meh, no.
-  7) What are potential issues? What could have gone wrong? What are the limitations?
    * presented interestingly and clearly? -> yes, meh, no.
    * convincing material provided? -> yes, meh, no.
-  8) Can you answer questions? Can you explain your choices? Can you defend your choices?
    * defended/discussed interestingly and clearly? -> yes, meh, no.
    * fitting and convincing extra material provided? -> yes, meh, no.
-  9) Bonus for something to think about/something creative: complex final chart, extra analysis, long-term implications, etc.
-  10) Bonus for something new/something I haven't seen before:  own analyses (e.g., not from this course), own explorations, other data/information, new methods, etc.
-  11) Bonus for something fun: funny, interesting, surprising, etc.






